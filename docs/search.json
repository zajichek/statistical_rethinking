[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Journey Through Statistical Rethinking",
    "section": "",
    "text": "Introduction\nThis is a repository containing my exhaustive notes and code from my journey through Richard McElreath’s Statistical Rethinking (Second Edition). Each chapter corresponds to the chapter in the book.\nHe also has a free course on GitHub that includes slides and a video playlist.\nAll source code for this project is available on GitHub in this repository. The book is deployed with GitHub Pages.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Software Configuration\nGoing through the Preface, we need to do some preliminary configuration to prepare ourselves to use the software in the book. I’m running a 16 inch MacBook Pro (Nov 2023) Apple M3 Pro chip, and macOS Sonoma 14.6.1.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#software-configuration",
    "href": "preface.html#software-configuration",
    "title": "Preface",
    "section": "",
    "text": "Steps to get setup\nAs the book suggests:\n\n“You will need to install both a C++ compiler (also called a”tool chain”) and the rstan package. Instructions for doing both are at mc-stan.org\n\n\n1. Installing the C++ toolchain\nFrom the main web page, we go to the instructions for installing RStan.\n\na. Install macrtools\nWe can use this R package to help setup the C++ toolchain.\n\n# install.packages(\"remotes\")\nremotes::install_github(\"coatless-mac/macrtools\")\n\nThen we run the following command:\n\nmacrtools::macos_rtools_install()\n\nI was prompted to enter a password (my computer’s password) in RStudio. After it installed, I received this message in the R console:\nCongratulations! \nXcode CLI, Gfortran, and R developer binaries have been installed successfully.\n\n\nb. Optimize compiler\nWe run the R code here to “enable some compiler optimizations to improve the estimation speed of the model”:\n\ndotR &lt;- file.path(Sys.getenv(\"HOME\"), \".R\")\nif (!file.exists(dotR)) dir.create(dotR)\nM &lt;- file.path(dotR, \"Makevars\")\nif (!file.exists(M)) file.create(M)\narch &lt;- ifelse(R.version$arch == \"aarch64\", \"arm64\", \"x86_64\")\ncat(paste(\"\\nCXX17FLAGS += -O3 -mtune=native -arch\", arch, \"-ftemplate-depth-256\"),\n    file = M, sep = \"\\n\", append = FALSE)\n\nIt ran without error (though I don’t really know what it did).\n\n\n\n2. Install rstan\nGoing back to the RStan page, we can run the following installation:\n\ninstall.packages(\"rstan\", repos = \"https://cloud.r-project.org/\", dependencies = TRUE)\n\nThis worked.\n\nTest some examples\nRunning the following example:\n\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\nI received a lot of output in the console (compilation and sampling).\n\nImport package\n\n## Load the package\nlibrary(\"rstan\") # observe startup messages\n\n# Set cores\noptions(mc.cores = parallel::detectCores()) # 12\n\n# Auto write compiled stan code\n# rstan_options(auto_write = TRUE) # &lt;-- Not doing it here, but would be useful\n\nIt says, “You will need to run these commands each time you load the rstan library.”\n\n\nRun an example model\nWe saved an example Stan file in stan/schools.stan, and then run the example:\n\n# Data to input into model\nschools_dat &lt;- \n  list(\n    J = 8, \n    y = c(28,  8, -3,  7, -1,  1, 18, 12),\n    sigma = c(15, 10, 16, 11,  9, 11, 10, 18)\n  )\n\n# Fit the model\nfit &lt;- stan(file = 'stan/schools.stan', data = schools_dat)\n\nIt fit with this message:\n\nWarning messages:\n1: There were 11 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them. \n2: Examine the pairs() plot to diagnose sampling problems\n\n\nprint(fit)\nplot(fit)\npairs(fit, pars = c(\"mu\", \"tau\", \"lp__\"))\n\nla &lt;- extract(fit, permuted = TRUE) # return a list of arrays \nmu &lt;- la$mu \n\n### return an array of three dimensions: iterations, chains, parameters \na &lt;- extract(fit, permuted = FALSE) \n\n### use S3 functions on stanfit objects\na2 &lt;- as.array(fit)\nm &lt;- as.matrix(fit)\nd &lt;- as.data.frame(fit)\n\n\n\n\n\nInstall book packages\n\ninstall.packages(c(\"coda\", \"mvtnorm\", \"devtools\", \"dagitty\"))\ndevtools::install_github(\"rmcelreath/rethinking\")\n\nHad issues with cmdstanr, so I’m following instructions here.\n\n# we recommend running this is a fresh R session or restarting your current session\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\nNow try again\n\ndevtools::install_github(\"rmcelreath/rethinking\")\n\nSuccess!\nAt this point, I don’t know what is better: rstan or cmdstanr",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "1.1 Models are robots\nStatistical models are (or can be) like golems in the sense that they are concerned with modeling the world (the truth), but they have no sense of morality on their own. They only do what is intended, regardless of consequences, implications, values, trade-offs, etc. It is up to us to direct the models towards what is useful, and use them in a way that makes sense–through logic, reasoning, common sense, etc. Therefore, we must separate the mathematical procedure of statistical modeling from the interpretation or implication that it has.\nIt’s often the case that statistics is taught straight from this golem point of view: people look at statistics as an objective, deterministic check list to do science. So they follow diagrams (like the test decision tree shown on page 2) to choose the “right” test, without considering any of the consequences of the golem that are only garnered through “wisdom”.\nMcElreath compares this to plumbers doing their jobs without knowing fluid dynamics, implying it can be “fine” in certain contexts. It’s when the user steps outside of the intended use by extrapolating: “It’s as if we got our hydraulic engineers by promoting plumbers” (3). Now I get the point there, but I think there probably is inherent knowledge that plumbers could contribute to the hydraulic engineering process, which seems to be an afterthought here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#models-are-robots",
    "href": "chapter1.html#models-are-robots",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "“Sometimes their underlying logic reveals implications previously hidden to their designers. These implications can be priceless discoveries. Or they may produce silly and dangerous behavior…there is no wisdom in the golem. It doesn’t discern when the context is inappropriate for its answers. It just knows its own procedures, nothing else. It just does as it’s told.” (2)\n\n\n\n“So students and many scientists tend to use charts like [the testing decision tree] without much thought to their underlying structure, without much awareness of the models that each procedure embodies, and without any framework to help them make the inevitable compromises required by real research. It’s not their fault.” (3)\n\n\n\n1.1.1 Good to play in everyone’s yard?\nOne common perk of statistics is that you can “play in everyone’s backyard”. It’s cool. But I’ve also more recently thought this to be a somewhat negative thing for producing truly useful statistical results. McElreath hints at that here as well, first in describing the inflexibility and fragility of classical statistical methods and how they are used so frequently, yet are rarely truly appropriate. More pointedly, he makes the case:\n\n“The point is that classical tools are not diverse enough to handle many common research questions. Every active area of science contends with unique difficulties of measurement and interpretation, converses with idiosyncratic theories in a dialect barely understood by other scientists from other tribes. Statistical experts outside the discipline can help, but they are limited by lack of fluency in the empirical and theoretical concerns of the discipline.” (3)\n\nThis is exactly right. Domain knowledge and intuition is king, and oftentimes statisticians (myself included) don’t truly have those things when approached for statistical assistance, limiting their ability to have optimal impact (though you may still be able to get the job done, if the goal is to say, just get a publication). I always say that a domain expert with some statistical chops is more effective than someone with tons of technical skills. That’s just the nature of it, so it’s about finding ways to collaborate with synergy to bring out what is needed from both sides.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#a-new-way-to-think",
    "href": "chapter1.html#a-new-way-to-think",
    "title": "1  The Golem of Prague",
    "section": "1.2 A new way to think",
    "text": "1.2 A new way to think\nHe sets the stage for the book’s thesis quite clearly:\n\n“So there are benefits in rethinking statistical inference as a set of strategies, instead of a set of pre-made tools.” (4)\n\nIt’s about statistical practice, of course, but also how we can teach statistics, even at an introductory level, differently so it’s not just based on objective, pre-made tools. This would be glorious. Instead of teaching students to systematically look up z-scores in the back of the text book and write out “significance” statements, teach them to solve problems with their brain. I’ve thought one simple way to start would just be to use varying (or random) confidence/significance levels on different problems, which at the very least would elicit some questioning and skepticism about what those numbers mean.\n\n1.2.1 Flaws in falsifiability\nHe discusses Karl Popper and the perceived objective of science being to falsify hypotheses. I agree that in principle, this makes sense: there’s never really a right answer, but we can move closer and closer to the truth by finding evidence of what it’s not. But I think one thing that’s overlooked is how this plays out in the real-world (and what I deduce as an undertone from McElreath): null hypotheses that we test against are arbitrary and blindly applied. We test default hypotheses that we don’t even think ourselves to be true (i.e., null effects). It’s just part of the statistical procedure (on page 5 in the purple side box he actually goes into this explicitly). So I think this is a component of it, but he then goes more into other reasons why this is flawed.\n\nHypotheses are not models: Getting at the idea that somehow each hypothesis tested is somehow its own thing that move us in some direction. But these are often actually just different subcomponents of the same phenomenon. Considering a model of the world for something, the angles that it is studied at (e.g., by different researchers), what is actually tested, how things are measured, etc. There is so much varible overlap that it’s impossible to make any conclusive statements about that true model that we believe exists.\nMeasurement matters: This is a huge point. Our study may suggest something. But you have a different group of people conduct the same study, there will be many nuances that likely differ. These things can always be critiqued, and cause real inconsistencies and variation–and rightly so. “They don’t trust the data. Sometimes they are right”. To me, this is one of the major flaws in current academic research when trying to confer individual implication to a study. There are literally infinite ways to conduct a study on the same topic.\n\nModels are always wrong, as he states, so how can we falsify one when we know whatever we choose next is wrong too? To me, this is a segway into Bayesian thinking. Instead of just looking at hypotheses in a traditional sense, we want to know what is most likely to have been the thing to created the data (out of all possibilities).\n\n“These hypotheses have vague boundries, because they begin as verbal conjectures, not precise models. There are hundreds of possible potential processes that [describe the null hypothesis].” (5)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#levels-of-hypotheses",
    "href": "chapter1.html#levels-of-hypotheses",
    "title": "1  The Golem of Prague",
    "section": "1.3 Levels of hypotheses",
    "text": "1.3 Levels of hypotheses\nDigging further into the hypotheses are not models point, he describes layers to framing the model:\n\nHypotheses: These are the vague things stated above\nProcess models: These are more specific possibilities of actual data generating processes (and causal structure) that could fit into those vague bins. These are the conceptual, time dependent (i.e., cause-effect ordering), causal structures of what may explain a phenomenon.\nStatistical models: We need to translate the causal model into some (associative) statistical model to be able to estimate things. The problem is that these models can be a result of multiple process models, even from different hypotheses.\n\nA key point here is that the statistical model (e.g., in this example) is what is represented by the statistical procedure/estimates, but that representation can be the same even with different data-generating processes, because the same data/predictions, for example, can be generated by completely different processes. Discussing typical hypothesis testing:\n\n“If we reject the null, we can’t really conclude that selection matters, because there are other neutral models that predict different distributions of alleles. And if we fail to reject the null, we can’t really conclude that evolution is neutral, because some selection models expect the same frequency distribution.” (7)\n\n\n1.3.1 What you should think about\n\n“If it turns out that all of the process models of interest make very similar predictions, then you know to search for a different description of the evidence, a description under which the processes look different.” (7)\n\nThis means that the way we represent, estimate, or quantify the process then should be changed because the way you’re currently doing it leads to ambiguity. So find different metrics, different representations, etc. Also, focus on the data-generating process:\n\n“Process models allow us to design statistical models with [e.g., unobserved variables and sampling bias] in mind. The statistical model alone is not enough.” (7)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#chapter1inching",
    "href": "chapter1.html#chapter1inching",
    "title": "1  The Golem of Prague",
    "section": "1.4 Inching towards Bayes",
    "text": "1.4 Inching towards Bayes\nThe first part of section 1.2.2 (and everthing leading up to it) hints at a Bayesian point of view.\n\n“In contrast, finding D tells us nothing certain about H, because other hypotheses might also predict D.” (7)\n\nThe idea is that many states of the world could very conceivably produce some observation, but that doesn’t mean that state of the world is therefore likely to be the one that did. This is very much related to the fallacy of the transposed conditional which I talk about here:\n\nand is a central point to one of my favorite books, The Cult of Statistical Significance and something I wrote about in a prior blog post.\n\n1.4.1 Measuring things\nThe all swans are white null example is useful as a reference point to compare with other important scientific hypotheses we are truly after. In this case, with such a “big” statement (applying to ALL swans) you just need to see one black one and it’s over and done with. But the problem, at least in part, is:\n\nNot all scientific inquiries are this simple, objective, and clear\nNot all measurements about the thing are clear either.\n\nFor example, what if the “black” swan you saw was actually a white swan that had a bad encounter with black paint or tar? Would you know? Maybe in that circumstance you would, but across all scientific inquiries, these nuances and possibilities for measurement error, bias, etc. are infinite.\n\n“At the edges of scientific knowledge, the ability to measure a hypothetical phenomenon is often in question as much as the phenomenon itself.” (8)\n\nThe example about neutrinos and the speed of light is a really great, intuitive example about the complexities and biases inherent to measurement. It’s people’s preconceived notion about what level of evidence is sufficient, mixed with the phenomenon being measured, the conditions under which it is being measured, and the fact that despite how “objective” we think a measurement device is, it is always an estimate of the real thing. Great section on page 8.\n\n“But the probabilistic nature of evidence rarely appears when practicing scientists discuss the philosophy and practice of falsification. My reading of the history of science is that these sorts of measurement problems are the norm, not the exception.” (9)\n\nIn discussing more opaque hypotheses,\n\n“Now, nearly everyone agrees that it is a good practice to design experiments and observations that can differentiate competeing hypotheses. But in many cases, the comparison must be probabilistic, a matter of degree, not kind.” (9)\n\nBoth of these hint at Bayesian thinking.\nThe last paragraph in section 1.2 is very important:\n\n“But falsification is always consensual, not logical. In light of real problems of measurement error and the continuous nature of natural phenomena, scientific communities argue towards consensus about the meaning of evidence.” (9)\n\nIt’s worth reading the rest of it. But the point being that we cannot objectively, and logical falsify something completely. It’s more so the consensus of people agree that, based on evidence, we all agree it is false. And the last sentence is key: “And it may hurt the public, by exaggerating the definitiveness of scientific knowledge.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#model-the-world",
    "href": "chapter1.html#model-the-world",
    "title": "1  The Golem of Prague",
    "section": "1.5 Model the world",
    "text": "1.5 Model the world\nIn order to take a new approach from falsification (and conform pre-existing golems to every situtation), we are to re-think everything in terms of models. Instead of applying a statistical test, we consider the model for how the data was generated–the data-generating process.\n\n“But if you are a good golem engineer, at least you’ll notice the destruction.” (10)\n\nThe point of this quote is to say that by focusing on the model, you focus on mechanism. You aren’t using the golem (statistical test/model) as a brute force objective procedure to “apply” to your data, but rather tailor a unique golem for the specific purpose of answering your research question. Thus it has to do with constructing the right model, informed by common sense, domain knowledge, causal pathways, etc. so that the interpretation side (in which the golem doesn’t have) is better positioned to answer your question. This means you have to dig deeper than is traditionally done, because this is how true science is done. You have to think. You have to know your model. You have to reason. Nothing about this is about mechanistically just thoughtlessly throwing data into modeling software, getting a value, following some rule, and writing a blanket statement about the results. Basically, you need to feel the result in your bones.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-tools-to-get-there",
    "href": "chapter1.html#the-tools-to-get-there",
    "title": "1  The Golem of Prague",
    "section": "1.6 The tools to get there",
    "text": "1.6 The tools to get there\nThe book’s primary focus is causal modeling through Bayesian statistics. An important thing to note is that he doesn’t use Bayesian modeling for ideological reasons, per say, it’s more so that it just happens to be the framework that makes sense to facilitate modeling from a generative perspective.\n\n1.6.1 Bayesian vs. frequentist\nHe simplifies Bayesian statistics as a way to count the ways things could happen, and then see what is most likely. And that’s a good way to think about it: we just want to know what is the most likely explanation for generating our observation, not how likely our data is under some hypothetical truth.\nIt’s also interesting he discusses frequentist statistics as a special case of Bayesian probability. He emphasizes that it’s based on the idea of imaginary resampling of data, and points out, and deservedly so, sure this could be a good way to think about it in a controlled lab experiment when you could conceivably keep sampling over and over, but for many other things, it just doesn’t make sense conceptually.\n\n“This resampling is never done, and in general it doesn’t even make sense–it is absurd to consider repeat sampling of the diversification of song birds in the Andes. As Sir Ronald Fisher, one of the most important frequentist statisticians of the twentieth century, put it: ‘the only populations that can be referred to in a test of significance have no objective reality, being exclusively the product of the statistician’s imagination.’” (11)\n\nQuite the interesting closing paragraph of page 11, stating that “Bayesian golems treat ‘randomness’ as a property of information, not of the world.”\n\n“Nothing in the real world–excepting controversial interpretations of quantum physics–is actually random. Presumably, if we had more information, we could exactly predict everything. We just use randomness to describe our uncertainty in the face of incomplete knowledge. From the perspective of our golem, the coin toss is ‘random’, but it’s really the golem that is random, not the coin.” (11)\n\nHe acknowledges the quantum point here. But that is precisely one of the main objections to the idea of “everything being predictable”. This is a potentially useful way to view a lot of things that are studied, because it makes a lot of sense, but probability fields in quantum physics seem to point to something deeper at a more fundamental level. This is explicitly discussed in the book, Light of the Mind, Light of the World: Illuminating Science Through Faith. It’s a central theme in the book, but Chapter 3 in particular discusses the “small gods” as being those who view the world from this mechanical philosophy: “a conviction that the world was made of bodies, whose movements and collisions were as sharply regulated as those displayed by an intricately calibrated machine…if they could be described using numbers alone, atoms might reveal the truth underlying all things” (page 69).\n\n\n1.6.2 Bayesianing, not Bayesianism\nHe further solidifies his stance that he’s not using Bayesian modeling on ideological grounds.\n\n“Note that the preceding description doesn’t invoke anyone’s ‘beliefs’ or subjective opinions. Bayesian data analysis is just a logical procedure for processing information. There is a tradition of using this procedure as a normative description of rational belief, a tradition called Bayesianism. But this book neither describes nor advocates it. In fact, I’ll argue that no statistical approach, Bayesian or otherwise, is by itself sufficient.” (12)\n\nHe then goes on to describe the advantage of Bayesian analysis–it is more intuitive. People can grasp the practical implications easier, and it just makes sense. Otherwise you are just trying to remember some steps or criteria or what boilerplate sentence to write about a p-value, without actually thinking through what it means. The interesting case he makes it how often a p-value (or confidence interval) is interpreted like a posterior probability (go back up the page, this is related to the fallacy of the transposed conditional as well), but the reverse is not done.\n\n“The opposite pattern of mistake–interpreting a posterior probability as a p-value–seems to happen only rarely. None of this ensures Bayesian analysis will be more correct…the scientist’s intuition will less commonly be at odds with actual logic of the framework.” (12)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#centering-the-model",
    "href": "chapter1.html#centering-the-model",
    "title": "1  The Golem of Prague",
    "section": "1.7 Centering the model",
    "text": "1.7 Centering the model\nAs I’ve been learning about statistical history, philosophy, and Bayesian thinking, one huge differentiator I’ve realized is the basic idea that in the Bayesian setting, the model becomes the center of attention, and the data is just there to supplement it. To me, this changes the entire view of the problem. I’ve talked about this before, like in this blog post, and in this supporting video:\n\nI bring this up because the first sentence of section 1.3.2, although stated casually, is very profound:\n\n“Bayesian data analysis provides a way for models to learn from data.” (13)\n\nThe emphasis being that our model can learn from data. Meaning our model is a constant, a living, breathing object that describes the world. We only use data, as it comes and goes, to help inform that model as it is needed.\n\n1.7.1 Beyond accuracy\nHe talks about the concept of overfitting, and the need for cross-validation, etc. But one attribute of Bayesian thinking that is related to this is the fact that multiple models can produce the same predictions, and so choosing the simpler one is in large part what Bayesian thinking does. So beyond getting good predictions, we’re chosing the model/mechanism that more likely generated the data. There is a fascinating paper related to this topic that discusses how Bayesian modeling is essentially the realization of Occam’s Razor–definitely worth the read.\n\n\n1.7.2 A multilevel approach\nHe implicitly describes multilevel models as a key concept in this book. Yes, these are models that we think of: individual students in classrooms, classrooms in schools, schools in district, etc. But the effect of using these models is profound:\n\n“Cross-validation and information criteria measure overfitting risk and help us to recognize it. Multilevel models actually do something about it.” (14)\n\nPartial pooling is what allows us to structure models and share information where it is necessary, while preserving variation and other important components of the data-generating process. In this sense, he extends this concept more generally to how we can think about multilevel modeling, in the context of missing data, factor analysis, etc. The concept that “each parameter can be thought of as a placeholder for a missing model” makes you realize the hierarchical structure of every modeling process.\nHe then makes a big claim: “multilevel regression deserves to be the default form of regression”. Stating that papers should have to justify not doing this, rather than the current practice. Intuitively, I think this makes sense. Part of the reason you can easily critique this in academic papers is because there are a lot of things worth critiquing, stemming from things like the cult of statistical significance, poor publication standards, reliance on “positive” p-values, and overall lack of accountability for anything that happens with the results beyond them being printed on a page, leaving open an infinite number of ways to “do” a study and get it published…but I digress.\n\n“Fitting and interpreting multilevel models can be considerably harder than fitting and interpreting a traditional regression model.” (15)\n\nExactly. It’s harder. And there’s no incentive to do it, because they’ll still get the funding and publication, so same end result.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter1.html#mechanism-and-causality",
    "href": "chapter1.html#mechanism-and-causality",
    "title": "1  The Golem of Prague",
    "section": "1.8 Mechanism and causality",
    "text": "1.8 Mechanism and causality\n\n“Facts outside the data are needed to decide which explanation is correct.” (16)\n\nHe makes a similar point here described above, that models can have similar predictions but not be the true causal mechanism. This has to do with overfitting (think the geocentric vs. heliocentric models), and inferring what makes sense logically regardless of the outputs of the model.\nThe concept of identification is very important here, because it distinguishes the simple act of predicting what will happen (i.e., I can predict that the wind is blowing given I see tree branches swaying) versus intervening on a process and inferring what will happen as a result (I can’t then go and move the branches around to make wind start blowing (besides the force generated from the branches which is different)).\n\n1.8.1 The diagram\nWe will use directed acyclic graphs (DAG) as the choice of graphical causal model (as this isn’t the only kind). They are not models themselves, but diagrams to help us think through what model and statistical procedures are most appropriate for the research question.\nIn the “Rethinking” box he makes the point about causal salad, which is just the idea that we can do causal inference by “adjusting for all confounders”, which is a very common practice in science, to just throw everything we can think of in a regression model and say we “controlled” for stuff. It’s curious why every one of those papers have a “limitation” section stating that causal inference can’t be assumed, like a disclaimer. Tells you that the people doing this probably don’t believe the results themselves, otherwise they’d be repressed to stand by such a statement.\n\nVideo Lecture From the Author",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "2.1 Unraveling the counting process\nAll models are and will always be wrong, that’s why they are called models. The “small world” represented by the model can only hope to be reflective of “large world”, the actual environment in which the model, insights, and inference that come out of it are used (i.e., where the rubber meets the road).\nThe “garden of forking paths” idea is about stretching out the process for counting things. Often in statistics, we just use formulas, or integrals, or whatever other notation to think through logic in our heads. This fundamental, systematic approach to counting lets us see what’s really going on behind the scenes. Ultimately, this leads to a more intuitive understanding of what Bayesian modeling is (in McElreath’s words): “really just counting and comparing of possibilities” (20).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "chapter2.html#unraveling-the-counting-process",
    "href": "chapter2.html#unraveling-the-counting-process",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "2.1.1 First, the likelihood\nIn the marble example, he starts by counting the number of ways a particular observed sequence of marble draws could have occurred given an assumed true state of the bag (i.e., a hypothesis, which he calls conjecture). Then proceeds to manually count (and draw) the possibilities–nothing fancy about it, just brute force. Once this count is obtained, the question becomes: what is this collection of counts for all other possible states of the bag?. This is getting at what Bayesian analysis fundamentally is: determining which state of the world is most likely given the evidence.\n\n“By comparing these counts, we have part of a solution for a way to rate the relative plausibility of each conjectured bag composition.” (23)\n\nThe word relative foreshadows many implications in Bayesian analysis, such as the focus on priors and likelihoods (not the normalizing constant), the different approaches to getting at the same answer (counts, rates, etc.), and the focus of comparing possible hypotheses relative to one another (and the whole slate of possibilities).\n\n2.1.1.1 Translating to familar notation\nSince I come at this from the “traditional” statistical point of view, it’s helpful to reconcile the approach here with the way I would typically think about it. Namely, we have a bag with four (4) marbles in it, each one being white or blue. Based on a sample from the bag, we want to infer what make up of all marbles is. Thus, the unknown parameter can be thought of as \\(p\\), the proportion of marbles that are blue (or white if we wanted). So, \\(p\\) can be one of five choices: \\(p \\in (0, .25, .5, .75, 1)\\). On page 21 it was stated that “a sequence of three marbles is pulled from the bag, one at a time, replacing the marble each time and shaking the bag before drawing another marble”–so we are assuming that we are selecting randomly with replacement (as a side note, a key point is our assumption: we are using our judgement that the sampling method (shaking randomly) is sufficient to assume random, independent draws, but this paradigm leaves the possibility to account for modeling steps where this assumption may not be valid).\nIf we take \\(N=3\\) draws and \\(X=2\\) is the number that were blue, then we can think of:\n\\(X \\sim Binomial(N=3, p)\\)\nAnd we get the following counts of possible ways to observe this sequence for each value of \\(p\\).\n\n\nCode\n# Possible values of p\np &lt;- c(0, .25, .5, .75, 1)\nmarbles &lt;- length(p) - 1\n\n# Number of samples\nN &lt;- 3\n\n# Observed sequence (1 = blue, 0 = white)\nX &lt;- c(1, 0, 1)\nX &lt;- sum(X)\n\n# How many ways could this happen for each p?\nways &lt;- (marbles * p)^X * (marbles * (1-p))^(N-X)\n\ncbind(p, ways, posterior_probability = ways / sum(ways))\n\n\n        p ways posterior_probability\n[1,] 0.00    0                  0.00\n[2,] 0.25    3                  0.15\n[3,] 0.50    8                  0.40\n[4,] 0.75    9                  0.45\n[5,] 1.00    0                  0.00\n\n\nThis matches what is in the book on page 23. Why? Well because there is a finite number of marbles, and we are re-scaling to be in absolute terms relative to that, instead of being in rate terms. The point is that we can think of the counting process in absolute magnitude (counts), relative (probabilities), or even likelihoods (basically, non-normalized probabilities). They will all produce the same posterior probability suggesting that the bag most likely has 3 blue marbles, because our calculation is relative to span of possible hypotheses (conjectures).\nHere we use the full binomial distribution for our calculations:\n\n\nCode\n# Compute traditional binomial probabilities\nbinom_probs &lt;- dbinom(x = X, size = N, prob = p)\ncbind(p, binom_probs, posterior_probability = binom_probs / sum(binom_probs))\n\n\n        p binom_probs posterior_probability\n[1,] 0.00    0.000000                  0.00\n[2,] 0.25    0.140625                  0.15\n[3,] 0.50    0.375000                  0.40\n[4,] 0.75    0.421875                  0.45\n[5,] 1.00    0.000000                  0.00\n\n\nNow we could even do it without it being normalized (i.e., we don’t care if it adds to 1):\n\n\nCode\n# Compute unnormalized probabilities\nunnorm_probs &lt;- p^X * (1-p)^(N-X)\ncbind(p, unnorm_probs, posterior_probability = unnorm_probs / sum(unnorm_probs))\n\n\n        p unnorm_probs posterior_probability\n[1,] 0.00     0.000000                  0.00\n[2,] 0.25     0.046875                  0.15\n[3,] 0.50     0.125000                  0.40\n[4,] 0.75     0.140625                  0.45\n[5,] 1.00     0.000000                  0.00\n\n\nThey all lead to same relative posterior probability (3 blue marbles).\nOne key caveat here that comes next: these results assume that each value of \\(p\\) is equally likely. But as he points out:\n\n“We may have additional information about the relative plausibility of each conjecture. This information could arise from knowledge of how the contens of the bag were generated.” (25)\n\nThis gets at the idea of a prior distribution. We may know something about how the bag was arranged, such that we have an inkling that a particular value of \\(p\\) is more likely to be true than the rest. In that case, what we have done above doesn’t work.\n\n\n\n2.1.2 Incorporating prior knowledge\nSince we’ve just established the number of ways to observe the original sequence of draws (blue, white, blue), we now consider that as our current model (or information) we know about the process. So if we took another draw, we can consider our current information as the prior knowledge, and combine that with the number of ways we could observe the next blue marble:\nOriginal\n\n\nCode\ncbind(p, ways, posterior_probability = ways / sum(ways))\n\n\n        p ways posterior_probability\n[1,] 0.00    0                  0.00\n[2,] 0.25    3                  0.15\n[3,] 0.50    8                  0.40\n[4,] 0.75    9                  0.45\n[5,] 1.00    0                  0.00\n\n\nNew\n\n\nCode\n# New marble drawn blue\nX_new &lt;- 1\n\n# Number of trials\nN_new &lt;- length(X_new)\n\n# How many ways could this happen for each p?\nways_new &lt;- (marbles * p)^X_new * (marbles * (1-p))^(N_new-X_new)\n\n# Number of ways to observe the new sequence, given our prior information\nprior &lt;- ways\nposterior_ways &lt;- ways_new * prior\n\ncbind(p, ways_new, prior, posterior_ways, posterior_probability = posterior_ways / sum(posterior_ways))\n\n\n        p ways_new prior posterior_ways posterior_probability\n[1,] 0.00        0     0              0            0.00000000\n[2,] 0.25        1     3              3            0.06521739\n[3,] 0.50        2     8             16            0.34782609\n[4,] 0.75        3     9             27            0.58695652\n[5,] 1.00        4     0              0            0.00000000\n\n\nNow this is sort of trivial, because this is the same as if we just thought of counting the full sequence of 4 draws:\n\n\nCode\n# Possible values of p\np &lt;- c(0, .25, .5, .75, 1)\nmarbles &lt;- length(p) - 1\n\n# Number of samples\nN &lt;- 4\n\n# Observed sequence (1 = blue, 0 = white)\nX &lt;- c(1, 0, 1, 1)\nX &lt;- sum(X)\n\n# How many ways could this happen for each p?\nways &lt;- (marbles * p)^X * (marbles * (1-p))^(N-X)\n\ncbind(p, ways, posterior_probability = ways / sum(ways))\n\n\n        p ways posterior_probability\n[1,] 0.00    0            0.00000000\n[2,] 0.25    3            0.06521739\n[3,] 0.50   16            0.34782609\n[4,] 0.75   27            0.58695652\n[5,] 1.00    0            0.00000000\n\n\nThis gets at something that was always murky in Bayesian updating: wouldn’t (or shouldn’t) your priors technically always reflect the most recent information, up to the very second? Meaning that if we set an initial prior, and then collected a single sample, and updated our model, shouldn’t the posterior of that model now be our prior for any subsequent estimate? Well, yes, and it turns out (as we’ve shown about) that these are equivalent ways to think about the mathematical procedure, because you are just enumerating the ways that your sequence of observations could have been observed. So the important piece is just capturing what you know about the parameters at the beginning, and also reflecting any additional information that comes about in the proceeding utilization of information.\nI could just wait until all my data is collected over some period of time, combine that with original priors, and get some estimate, or I could sequentially run my estimation procedure each time as a new observation comes in, with everything prior to that being reflected in a prior–it is the same thing. The key is our assumption about how that intermediate information is being generated sequentially–if we learn additional information after say, the 10th sample, that was never known before, we can then incorporate that into our estimation at that point in time.\n\n“This updating approach amounts to nothing more than asserting that (1) when we have previous information suggesting there are W ways for a conjecture to produce a previous observation D and (2) we acquire new observations _D*_ that the same conjecture can produce in _W*_ ways, then (3) the number of ways the conjecture can account for both D as well as _D*_ is just the produce W X _W*_…Multiplication is just a shortcut to enumerating and counting up all of the paths through the garden that counld produce all the observations.” (25)\n\nThe example he gives to demonstrate the point on “additional information in between draws” is if suddenly you knew something about how the bags of marbles are produced (which is new data in a different form), which implies that there will be no bags containing all white or all blue marbles, three times as many containing only one blue marble and two times as many containing two blue marbles, both vs. one white marble, then we can now updating our current knowledge from the draws we’ve already done with this new information:\n\n\nCode\n# What we know NOW\nprior &lt;- posterior_ways\n\n# Relative counts of the conjectures\ncounts &lt;- c(0, 3, 2, 1, 0)\n\n# New ways\nways_new &lt;- prior * counts\n\ncbind(p, prior, counts, ways_new, posterior_probability = ways_new / sum(ways_new))\n\n\n        p prior counts ways_new posterior_probability\n[1,] 0.00     0      0        0             0.0000000\n[2,] 0.25     3      3        9             0.1323529\n[3,] 0.50    16      2       32             0.4705882\n[4,] 0.75    27      1       27             0.3970588\n[5,] 1.00     0      0        0             0.0000000\n\n\nNotice that if we had known about the bag proportions before we did any draws, that would have been the more intuitive prior distribution to start with, yet we would have reached the same answer. We’re just enumerating possibilities based on the information we receive.\nIn this vain, he rarely recommends that we would use a “principle of indifference” (i.e., act like we know nothing about the parameters beforehand, and treat them all as equally likely):\n\n“This book does not use nor endorse”ignorance” priors. As we’ll see in later chapters, the structure of the model and the scientific context always provide information that allows us to do better than ignorance.” (26)\n\nThis is obviously true. There is no circumstance where we should claim we know absolutely nothing about. Even if it is just a little, include it in the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "chapter2.html#building-a-model",
    "href": "chapter2.html#building-a-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Building a model",
    "text": "2.2 Building a model\nHe characterizes the model building process into three steps, which supports our previous profundity about Bayesian thinking: switching from thinking about the data to the model, and only use data as a fleeting piece of information to fine tune that model. The three steps are:\n\nData story: Thinking about the data-generating process (no data included)\nUpdate: Use your data to now revise, or as he says, “educate” your model based on evidence\nEvaluate: That is your model as of now, but up to this point in time, we may need to change it\n\n\n“But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data.” (28)\n\nThis is profound. Because once we specify the data-generating process, we think we now know (to the best of our knowledge), how an subsequent data would be generated. Thus, we don’t actually need real data to begin evaluating the truth to our model. We can start simulating it and see how it plays out. If stuff starts to look how it does it real life, we may be on the right track, and then could move forward with getting real data to inform our estimates.\n\n“When you are forced to consider sampling and measurement and make a precise statement of how temperature predicts ran, many stories and resulting models will be consistent with the same vague hypothesis. Resolving that ambiguity often leads to important realizations and model revisions, before any model is fit to data.” (29)\n\nThis is exactly my point I made in Section 1.2.1. There are infinite ways to procedurally carry out a study answering the same question (i.e., how you define things, how data is collected (exactly), every detailed assumption made along the way, etc.). All of these nuances can have drastic effects on the ultimate conclusions made.\n\n2.2.1 Updating priors\nHe goes through the concept of what Bayesian updating is to get a feel for it. We can recreate Figure 2.5 (page 30) to better intuit what is happening behind the scenes.\n\n\nCode\nlibrary(tidyverse)\n\n# The observed sample sequence\nobserved_sample &lt;- c(1, 0, 1, 1, 1, 0, 1, 0, 1) # 1 = water; 0 = land\nN &lt;- length(observed_sample)\n\n# Approximate the set of inifinite p-values by a large set of discrete ones\np_continuous &lt;- seq(0, 1, .01)\n\n# Set the prior probability (uniform over the possibly choices)\nprior &lt;- rep(1 / length(p_continuous), length(p_continuous))\n\n# Set the current posterior as the prior (before any data collected)\nlast_posterior &lt;- prior\n\n# Make result set\nresults &lt;- tibble()\n\n# For each value in the observed sample \nfor(i in 1:N) {\n  \n  # 1. Get the sub-sample\n  sub_sample &lt;- observed_sample[1:i]\n  \n  # 2. Compute metrics (the number of water samples, and the total number of spins)\n  W_temp &lt;- sum(sub_sample)\n  N_temp &lt;- length(sub_sample)\n  \n  # 3. Compute the likelihood for each p\n  temp_likelihood &lt;- p_continuous^W_temp * (1 - p_continuous)^(N_temp - W_temp)\n  \n  # 4. Posterior\n  temp_posterior &lt;- temp_likelihood / sum(temp_likelihood)\n  \n  # 5. Add to results\n  results &lt;-\n    results |&gt;\n    bind_rows(\n      tibble(\n        sample = i,\n        sequence = paste(sub_sample, collapse = \",\"),\n        p_continuous,\n        likelihood = temp_likelihood,\n        current = temp_posterior,\n        last = last_posterior\n      )\n    )\n  \n  # Set the new last posterior\n  last_posterior &lt;- temp_posterior\n  \n}\n\nresults |&gt;\n  \n  # Send down the rows\n  pivot_longer(\n    cols = c(last, current)\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_area(\n    aes(\n      x = p_continuous,\n      y = value,\n      fill = name\n    ),\n    color = \"black\",\n    alpha = .65,\n    position = \"identity\"\n  ) +\n  facet_wrap(\n    ~paste0(\"Spin: \", factor(sample), \" \\nSample: \", sequence)\n  ) +\n  theme(\n    legend.position = \"top\",\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(colour = \"gray\"),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  xlab(\"p\") +\n  ylab(\"Posterior Probability\") +\n  labs(\n    fill = \"Posterior\"\n  ) +\n  scale_fill_manual(\n    values = c(\"blue\", \"darkgray\")\n  ) \n\n\n\n\n\n\n\n\n\nNote that this is actually just a discrete approximation of the true continuous, analytical solution, but it covers the idea of what is going on. I previously went in depth on this specific problem from the book here which provides much more detail and code.\n\n“Notice that every updated set of plausibilities becomes the initial plausibilities for the next observation. Every conclusion is the starting point for future inference.” (31)\n\nThis supports the idea that our model is the living, breathing thing, and that we should use everything we know up to now to inform it. Then use new information to subsequently update it as time goes on. Our model is never complete.\nAnother crucial point about considering sequential updates versus using your entire data set in a single updating step:\n\n“So the data could be presented to your model in any order, or all at once even. In most cases, you will present the data all at once, for the sake of convenience. But it’s important to realize that this merely represents abbreviation of an iterated learning process.” (31)\n\nThere is a catch though, as reflected later:\n\n“That is only true, however, because the model assumes that order is irrelevant to inference. When something is irrelevant to the machine, it won’t affect the inference directly. But it may affect it indirectly, because the data will depend upon order.” (31)\n\nHence the importance of understanding the data-generating process.\n\n\n2.2.2 Bayesian models need no data\nIn the Rethinking box on page 31, he discusses a crucial point related to what we’ve talked about: that there is no sample size requirement for Bayesian inference. This again is a key reason why this paradigm is superior. We aren’t stuck in asymptotics and arbitrary sample size cutoffs (e.g., N=30). We can specify our model, with prior information, using the best available knowledge we have up to this point in time, and actually get reliable estimates. Then as we collect a single observation, our new (posterior) estimates will nicely update to reflect the (likely little) amount of information contained in it, instead of needing to gather a bunch of data in order to get even get something interpretable. I previously discussed this in depth with an example here:\n\n\n\n2.2.3 Tie inferences back to what matters\nIt’s not enough to just provide an estimate and say “it matters” or not, like we arbitrarily do with frequentist p-values and 5% thresholds. These are empty statements. And again, for a result to be meaningful, we need to feel it in our bones. If all results were presented with such relevance, there wouldn’t be so much controversy about the value of “science”. This problem in large part in our society is due to the lacksidasical approach to assigning meaning to estimates, declaring scientific certainty, and not actually doing to the due diligence of acknowledging nuances to why some things matter to some people and some don’t.\nThis point is hit on in the Evaluate section (2.2.3) of the book:\n\n“Instead, the objective is to check the model’s adequacy for some purpose. This usually means asking and answering additional questions, beyond those that originally constructed the model. Both the questions and answers will depend upon the scientific context.” (32)\n\nWhen we just blindly run statistical tests, get a p-value less than 5%, and then declare that “science shows this thing, and anyone who disregards it is anti-science”, we’ve entered into something that is more religious, not scientific. Because we haven’t actually convinced the person that these results matter. It is our job to frame scientific questions, their scope, and the subsequent results/interpretation closer something tangible that makes people feel the results.\nAs the great book The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives says:\n\n“Real science changes one’s mind. That’s one way to see that the proliferation of unpersuasive significance tests is not real science.” (page 112)\n\nand\n\n“She can test her belief in the price effect by looking at the magnitudes, using, for example, the highly advanced technique common in data-heavy articles in physics journals: ‘interocular trauma’. That is, she can look and see if the result hits her between the eyes.” (page 72)\n\nSee a much more extensive discussion of this book, and a large list of quotes from that book that I love, here.\n\n\n2.2.4 Is Bayesian modeling subjective?\nA common critique of Bayesian modeling is that it is inherently subjective, because people choose their priors. I once thought of it that way as well, until I listed to this episode of Learning Bayesian Statistics. Frank Harrell argues, and I now agree, that it is even less subjective because we are actually accounting for what we know, not just being forced into blind assumptions that occur in the frequentist paradigm. This is emphasized on page 35:\n\n“None of this should be understood to mean that any statistical analysis is not inherently subjective, because of course it is–lots of little subjective decisions are involved in all parts of science. It’s just that priors and Bayesian data analysis are no more inherently subjective than are likelihoods and the repeat sampling assumptions required for significance testing…No one is required to swear an oath to the assumptions of a model, and no set of assumptions deserves our obedience.” (35)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "chapter2.html#making-your-model-work",
    "href": "chapter2.html#making-your-model-work",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.3 Making Your Model Work",
    "text": "2.3 Making Your Model Work\n\n“However, knowing the mathematical rule is often of little help, because many of the interesting models in contemporary science cannot be conditioned formally, no matter your skill in mathematics. And while some broadly useful models like linear regression can be conditioned formally, this is only possible if you constrain your choice of prior to special forms that are easy to do mathematics with. We’d like to avoid forced modeling choices ot this kind, instead favoring conditioning engines that can accomodate whichever prior is most useful for inference” (39)\n\nOne challenge in Bayesian analysis, and having the theoretical flexibility for specifying priors (and models in general) that best represent what you know, is that it doesn’t always work out “nicely” mathematically. Most common statistical methods are used because the math works out, so we can have software that fits things according to that assumed structure. Similarly, in the Bayesian framework, for certain models, there are “nice” choices you can make for prior that make the posterior work out analytically (closed form solutions), which makes it easier to deal.\nBut we don’t necessarily want to restrict ourselves to choosing model structures and assumptions for the sake of mathematical convenience. We want to specify our models to reflect what we actually believe to be true. That means we may have very messy functions pulled out of our brain (e.g., step functions, weird shapes, etc.). But if that’s what makes most sense scientifically, we want to have flexible estimating engines to allow us to do this and implement it. That’s what motivates the need for things he talks about next, like markov chain monte carlo (MCMC) simulation or quadratic approximation. These things let us focus on specifying models that we think make sense, regardless how the math plays out downstream. He calls these conditioning engines.\n\n2.3.1 Grid approximation\nThis is what we did above. We know that the true parameter value (\\(p\\)) could be anything (as it is continuous), but we approximated what the posterior would look like by chopping up its domain into small intervals and computing pseudo-discrete posteriors based on that. The yielding plots reflect approximately what the actual posterior would look like if we derived the full analytical solution. I think this is a great way to do it, at least to start, because you can get into the “ballpark” of your posterior, and if for some reason that ends up not being specific enough to satisfy your inquiry, you can look to more complex methods to refine your estimates in a more complete way.\n\n\n2.3.2 Quadratic approximation\n\n“Under quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian. This means the posterior distribution can be usefully approximated by a Gaussian distribution.” (42)\n\nWhen we get more parameters in the model, we can’t be specifying large grids of parameters to approximate, so we need something else. The quadratic approximation takes advantage of properties of posteriors (that, locally, they tend to be Gaussian), and that Gaussian distributions can be described by only the mean and variance.\n\n\nCode\nlibrary(rethinking)\n\n\nLoading required package: cmdstanr\n\n\nWarning: package 'cmdstanr' was built under R version 4.4.2\n\n\nThis is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- Use set_cmdstan_path() to set the path to CmdStan\n\n\n- Use install_cmdstan() to install CmdStan\n\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nLoading required package: parallel\n\n\nrethinking (Version 2.42)\n\n\n\nAttaching package: 'rethinking'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n\n\nCode\nglobe.pa &lt;- quap(\n  alist(\n    W ~ dbinom(W+L, p),\n    p ~ dunif(0,1)\n  ),\n  data = list(W = 6, L = 3)\n)\nprecis(globe.pa)\n\n\n      mean        sd     5.5%    94.5%\np 0.666667 0.1571337 0.415537 0.917797\n\n\n\n\n2.3.3 Markov chain Monte Carlo (MCMC)\n\n“The conceptual challenge of MCMC lies in its highly non-obvious strategy. Instead of attempting to compute or approximate the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, and the frequencies of these values correspond to the posterior plauibilities.” (45)\n\n\nVideo Lecture From the Author",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Sampling the Imaginary",
    "section": "",
    "text": "Some notes…\n\nVideo Lecture From the Author (start at 45:17)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  }
]